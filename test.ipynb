{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T11:38:42.502291Z",
     "start_time": "2025-05-20T11:38:42.489343Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from data import ImageDataset\n",
    "from training_pipeline import worker_init_fn, set_seed\n",
    "from models import Generator, Discriminator\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "random_seed = 42\n",
    "set_seed(random_seed)"
   ],
   "id": "19d1e14aab842e97",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T11:16:18.429584Z",
     "start_time": "2025-05-20T11:16:13.538121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from data import split_dataset\n",
    "\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "# enough to run once (splitting data)\n",
    "\n",
    "source_path = \"data/cats/Data\"\n",
    "destination_path = \"data/cats/split_data\"\n",
    "split_dataset(source_path, destination_path, train_ratio, val_ratio, test_ratio, random_seed)"
   ],
   "id": "9282e94643123ee8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\cats\\split_data\\train\n",
      "data\\cats\\split_data\\val\n",
      "data\\cats\\split_data\\test\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-20T11:38:45.207037Z",
     "start_time": "2025-05-20T11:38:44.483496Z"
    }
   },
   "source": [
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)), # to set (64, 64) by default\n",
    "#     transforms.ToTensor()\n",
    "# ])\n",
    "\n",
    "img_size = 64\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "train_path = \"data/cats/split_data/train\"\n",
    "val_path = \"data/cats/split_data/val\"\n",
    "test_path = \"data/cats/split_data/test\"\n",
    "\n",
    "train_dataset = ImageDataset(train_path, transform=transform)\n",
    "val_dataset = ImageDataset(val_path, transform=transform)\n",
    "test_dataset = ImageDataset(test_path, transform=transform)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 2048\n",
    "n_workers = 4\n",
    "prefetch_factor = 4 if n_workers > 0 else None\n",
    "persistent_workers = True if n_workers > 0 else False\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=n_workers, pin_memory=True, prefetch_factor=prefetch_factor,persistent_workers=persistent_workers, worker_init_fn=worker_init_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=n_workers, pin_memory=True, prefetch_factor=prefetch_factor, persistent_workers=persistent_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=n_workers, pin_memory=True, prefetch_factor=prefetch_factor, persistent_workers=persistent_workers)"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## DCGAN",
   "id": "6171fa82135f2e99"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T11:38:46.269422Z",
     "start_time": "2025-05-20T11:38:46.264412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "latent_dim = 100\n",
    "num_epochs = 100\n",
    "lr = 0.0002\n",
    "beta1 = 0.5"
   ],
   "id": "585717cd0e1aaffd",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T11:38:46.981440Z",
     "start_time": "2025-05-20T11:38:46.935006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "netG = Generator(latent_dim).to(device)\n",
    "netD = Discriminator().to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "fixed_noise = torch.randn(img_size, latent_dim, device=device)"
   ],
   "id": "56995f612e9b2599",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T11:45:48.938899Z",
     "start_time": "2025-05-20T11:38:48.009317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, real_imgs in enumerate(train_loader):\n",
    "        real_imgs = real_imgs.to(device)\n",
    "        b_size = real_imgs.size(0)\n",
    "\n",
    "        # Real and fake labels\n",
    "        real_labels = torch.ones(b_size, device=device)\n",
    "        fake_labels = torch.zeros(b_size, device=device)\n",
    "\n",
    "        ## Update Discriminator ##\n",
    "        netD.zero_grad()\n",
    "\n",
    "        output_real = netD(real_imgs)\n",
    "        loss_real = criterion(output_real, real_labels)\n",
    "\n",
    "        noise = torch.randn(b_size, latent_dim, device=device)\n",
    "        fake_imgs = netG(noise)\n",
    "        output_fake = netD(fake_imgs.detach())\n",
    "        loss_fake = criterion(output_fake, fake_labels)\n",
    "\n",
    "        loss_D = loss_real + loss_fake\n",
    "        loss_D.backward()\n",
    "        optimizerD.step()\n",
    "\n",
    "        ## Update Generator ##\n",
    "        netG.zero_grad()\n",
    "        output = netD(fake_imgs)\n",
    "        loss_G = criterion(output, real_labels)  # Try to fool the discriminator\n",
    "        loss_G.backward()\n",
    "        optimizerG.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] Step [{i}] Loss_D: {loss_D:.4f} Loss_G: {loss_G:.4f}\")\n",
    "\n",
    "    # Save generated images\n",
    "    with torch.no_grad():\n",
    "        fake = netG(fixed_noise).detach().cpu()\n",
    "        save_image(fake, f\"output/fake_epoch_{epoch+1:03d}.png\", normalize=True)"
   ],
   "id": "113e7217a79b35ca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] Step [0] Loss_D: 1.3608 Loss_G: 2.5621\n",
      "Epoch [2/100] Step [0] Loss_D: 0.0799 Loss_G: 6.8304\n",
      "Epoch [3/100] Step [0] Loss_D: 0.0501 Loss_G: 9.3847\n",
      "Epoch [4/100] Step [0] Loss_D: 0.1133 Loss_G: 16.7177\n",
      "Epoch [5/100] Step [0] Loss_D: 0.3299 Loss_G: 9.5806\n",
      "Epoch [6/100] Step [0] Loss_D: 0.7714 Loss_G: 11.0277\n",
      "Epoch [7/100] Step [0] Loss_D: 0.8257 Loss_G: 9.2877\n",
      "Epoch [8/100] Step [0] Loss_D: 0.1186 Loss_G: 4.1836\n",
      "Epoch [9/100] Step [0] Loss_D: 1.0004 Loss_G: 3.9597\n",
      "Epoch [10/100] Step [0] Loss_D: 0.5586 Loss_G: 4.1883\n",
      "Epoch [11/100] Step [0] Loss_D: 0.5469 Loss_G: 2.7442\n",
      "Epoch [12/100] Step [0] Loss_D: 1.1111 Loss_G: 1.5368\n",
      "Epoch [13/100] Step [0] Loss_D: 0.7199 Loss_G: 2.5120\n",
      "Epoch [14/100] Step [0] Loss_D: 1.9684 Loss_G: 7.4636\n",
      "Epoch [15/100] Step [0] Loss_D: 0.8633 Loss_G: 3.4412\n",
      "Epoch [16/100] Step [0] Loss_D: 0.7176 Loss_G: 2.1885\n",
      "Epoch [17/100] Step [0] Loss_D: 0.5947 Loss_G: 2.4761\n",
      "Epoch [18/100] Step [0] Loss_D: 0.8008 Loss_G: 3.0216\n",
      "Epoch [19/100] Step [0] Loss_D: 0.9925 Loss_G: 4.2979\n",
      "Epoch [20/100] Step [0] Loss_D: 0.8981 Loss_G: 3.2392\n",
      "Epoch [21/100] Step [0] Loss_D: 0.7531 Loss_G: 2.2494\n",
      "Epoch [22/100] Step [0] Loss_D: 0.6918 Loss_G: 1.9953\n",
      "Epoch [23/100] Step [0] Loss_D: 0.9205 Loss_G: 2.9101\n",
      "Epoch [24/100] Step [0] Loss_D: 0.8291 Loss_G: 1.5699\n",
      "Epoch [25/100] Step [0] Loss_D: 1.0511 Loss_G: 3.4198\n",
      "Epoch [26/100] Step [0] Loss_D: 1.3184 Loss_G: 1.2061\n",
      "Epoch [27/100] Step [0] Loss_D: 0.9184 Loss_G: 1.5148\n",
      "Epoch [28/100] Step [0] Loss_D: 0.9518 Loss_G: 3.4852\n",
      "Epoch [29/100] Step [0] Loss_D: 1.4342 Loss_G: 4.2507\n",
      "Epoch [30/100] Step [0] Loss_D: 0.7859 Loss_G: 2.7297\n",
      "Epoch [31/100] Step [0] Loss_D: 0.8676 Loss_G: 2.6624\n",
      "Epoch [32/100] Step [0] Loss_D: 1.1102 Loss_G: 1.5770\n",
      "Epoch [33/100] Step [0] Loss_D: 1.1957 Loss_G: 4.5639\n",
      "Epoch [34/100] Step [0] Loss_D: 1.1581 Loss_G: 1.3478\n",
      "Epoch [35/100] Step [0] Loss_D: 0.7447 Loss_G: 3.0194\n",
      "Epoch [36/100] Step [0] Loss_D: 1.2827 Loss_G: 2.1569\n",
      "Epoch [37/100] Step [0] Loss_D: 0.9732 Loss_G: 1.9049\n",
      "Epoch [38/100] Step [0] Loss_D: 0.9842 Loss_G: 1.8005\n",
      "Epoch [39/100] Step [0] Loss_D: 2.0123 Loss_G: 0.8395\n",
      "Epoch [40/100] Step [0] Loss_D: 1.1291 Loss_G: 2.4275\n",
      "Epoch [41/100] Step [0] Loss_D: 0.9672 Loss_G: 2.3272\n",
      "Epoch [42/100] Step [0] Loss_D: 0.8693 Loss_G: 2.2337\n",
      "Epoch [43/100] Step [0] Loss_D: 0.9299 Loss_G: 2.6820\n",
      "Epoch [44/100] Step [0] Loss_D: 0.9435 Loss_G: 1.6209\n",
      "Epoch [45/100] Step [0] Loss_D: 0.8006 Loss_G: 2.0109\n",
      "Epoch [46/100] Step [0] Loss_D: 0.8935 Loss_G: 2.0920\n",
      "Epoch [47/100] Step [0] Loss_D: 0.9586 Loss_G: 1.5438\n",
      "Epoch [48/100] Step [0] Loss_D: 0.8305 Loss_G: 2.3625\n",
      "Epoch [49/100] Step [0] Loss_D: 1.1335 Loss_G: 2.6963\n",
      "Epoch [50/100] Step [0] Loss_D: 0.8536 Loss_G: 2.8002\n",
      "Epoch [51/100] Step [0] Loss_D: 0.7755 Loss_G: 2.6646\n",
      "Epoch [52/100] Step [0] Loss_D: 0.9827 Loss_G: 1.6230\n",
      "Epoch [53/100] Step [0] Loss_D: 0.7055 Loss_G: 2.9321\n",
      "Epoch [54/100] Step [0] Loss_D: 1.1046 Loss_G: 2.2480\n",
      "Epoch [55/100] Step [0] Loss_D: 0.8478 Loss_G: 2.6871\n",
      "Epoch [56/100] Step [0] Loss_D: 0.9145 Loss_G: 2.3167\n",
      "Epoch [57/100] Step [0] Loss_D: 0.6646 Loss_G: 2.6994\n",
      "Epoch [58/100] Step [0] Loss_D: 0.9394 Loss_G: 1.5695\n",
      "Epoch [59/100] Step [0] Loss_D: 1.0546 Loss_G: 1.0531\n",
      "Epoch [60/100] Step [0] Loss_D: 2.0403 Loss_G: 1.4709\n",
      "Epoch [61/100] Step [0] Loss_D: 0.6960 Loss_G: 2.8648\n",
      "Epoch [62/100] Step [0] Loss_D: 0.9189 Loss_G: 1.6116\n",
      "Epoch [63/100] Step [0] Loss_D: 0.7554 Loss_G: 2.0543\n",
      "Epoch [64/100] Step [0] Loss_D: 0.7321 Loss_G: 2.4936\n",
      "Epoch [65/100] Step [0] Loss_D: 0.9512 Loss_G: 2.9558\n",
      "Epoch [66/100] Step [0] Loss_D: 1.0269 Loss_G: 1.6302\n",
      "Epoch [67/100] Step [0] Loss_D: 1.3978 Loss_G: 0.8392\n",
      "Epoch [68/100] Step [0] Loss_D: 0.8151 Loss_G: 2.5982\n",
      "Epoch [69/100] Step [0] Loss_D: 0.8568 Loss_G: 3.1821\n",
      "Epoch [70/100] Step [0] Loss_D: 0.9351 Loss_G: 2.2701\n",
      "Epoch [71/100] Step [0] Loss_D: 1.2143 Loss_G: 4.0152\n",
      "Epoch [72/100] Step [0] Loss_D: 0.8584 Loss_G: 2.9453\n",
      "Epoch [73/100] Step [0] Loss_D: 1.2842 Loss_G: 0.9970\n",
      "Epoch [74/100] Step [0] Loss_D: 0.7213 Loss_G: 2.8948\n",
      "Epoch [75/100] Step [0] Loss_D: 0.7026 Loss_G: 2.6260\n",
      "Epoch [76/100] Step [0] Loss_D: 1.2410 Loss_G: 1.7116\n",
      "Epoch [77/100] Step [0] Loss_D: 1.6127 Loss_G: 0.9787\n",
      "Epoch [78/100] Step [0] Loss_D: 0.7205 Loss_G: 2.8438\n",
      "Epoch [79/100] Step [0] Loss_D: 1.2901 Loss_G: 1.4213\n",
      "Epoch [80/100] Step [0] Loss_D: 0.5952 Loss_G: 3.2407\n",
      "Epoch [81/100] Step [0] Loss_D: 1.1542 Loss_G: 4.8491\n",
      "Epoch [82/100] Step [0] Loss_D: 2.1377 Loss_G: 1.2029\n",
      "Epoch [83/100] Step [0] Loss_D: 0.8515 Loss_G: 2.2692\n",
      "Epoch [84/100] Step [0] Loss_D: 0.7470 Loss_G: 2.9110\n",
      "Epoch [85/100] Step [0] Loss_D: 0.8122 Loss_G: 2.5365\n",
      "Epoch [86/100] Step [0] Loss_D: 0.6746 Loss_G: 1.9432\n",
      "Epoch [87/100] Step [0] Loss_D: 1.0058 Loss_G: 4.2022\n",
      "Epoch [88/100] Step [0] Loss_D: 1.5541 Loss_G: 4.6409\n",
      "Epoch [89/100] Step [0] Loss_D: 1.1740 Loss_G: 4.2806\n",
      "Epoch [90/100] Step [0] Loss_D: 0.5399 Loss_G: 3.0475\n",
      "Epoch [91/100] Step [0] Loss_D: 0.9982 Loss_G: 1.3146\n",
      "Epoch [92/100] Step [0] Loss_D: 0.8080 Loss_G: 4.5773\n",
      "Epoch [93/100] Step [0] Loss_D: 0.4396 Loss_G: 2.8316\n",
      "Epoch [94/100] Step [0] Loss_D: 1.1481 Loss_G: 3.1069\n",
      "Epoch [95/100] Step [0] Loss_D: 0.7731 Loss_G: 4.2340\n",
      "Epoch [96/100] Step [0] Loss_D: 0.3882 Loss_G: 2.8374\n",
      "Epoch [97/100] Step [0] Loss_D: 0.4271 Loss_G: 2.5081\n",
      "Epoch [98/100] Step [0] Loss_D: 0.7502 Loss_G: 1.9783\n",
      "Epoch [99/100] Step [0] Loss_D: 1.2570 Loss_G: 5.1876\n",
      "Epoch [100/100] Step [0] Loss_D: 0.8903 Loss_G: 3.8049\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a98ea051a8af1711"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
